Build the Image classification model by dividing the model into following 4 stages:
a. Loading and preprocessing the image data
b. Defining the model’s architecture
c. Training the model
d. Estimating the model’s performance

///// code explaination
You've successfully built, trained, and evaluated your first feedforward neural network\! Here is a complete, step-by-step explanation of everything your code did.

-----

### Setup: Downloading Packages

```python
!pip install tensorflow
!pip install matplotlib
```

This is the initial setup.

  * **`!pip install tensorflow`**: This command installs TensorFlow, which is the main library (the "engine") that provides all the tools for building and training machine learning models. Keras is included within TensorFlow.
  * **`!pip install matplotlib`**: This installs a popular library for creating graphs and plots, which you use at the end to visualize your results.
  * The **"Requirement already satisfied"** messages just mean you already have these libraries installed, so `pip` didn't need to do anything.

-----

### a. Importing Necessary Packages

```python
import tensorflow as tf
from tensorflow import keras 
import matplotlib.pyplot as plt
import random
```

Here, you're "loading" the installed libraries into your script so you can use their functions.

  * **`import tensorflow as tf`**: Imports the core TensorFlow library. The `as tf` part is a standard, short nickname.
  * **`from tensorflow import keras`**: Imports the `keras` module from TensorFlow. Keras is the user-friendly API you use to *build* the model (define layers, etc.).
  * **`import matplotlib.pyplot as plt`**: Imports the plotting library, nicknamed `plt`.
  * **`import random`**: Imports Python's built-in library for generating random numbers. You use this to pick a random test image later.

> **Note on Warnings:** The `UserWarning: Protobuf gencode version...` messages are not errors. They are just compatibility notes from Google's underlying data format. For this project, **they can be safely ignored.**

-----

### b. Load and Prepare the Data

```python
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train / 255
x_test = x_test / 255
```

This is a critical data preparation step.

1.  **`mnist = tf.keras.datasets.mnist`**: This points to the MNIST dataset, which is conveniently built into Keras.
2.  **`(x_train, y_train), (x_test, y_test) = mnist.load_data()`**: This one line does two things:
      * It downloads the MNIST dataset (which you saw the progress bar for).
      * It splits the data into two "buckets":
          * **Training Set (`x_train`, `y_train`):** A set of 60,000 images (`x_train`) and their corresponding correct labels (`y_train`). The model gets to "study" these.
          * **Testing Set (`x_test`, `y_test`):** A separate set of 10,000 images (`x_test`) and their labels (`y_test`). The model *never* trains on this data; it's used only at the end to see how well the model learned (like a final exam).
3.  **`x_train = x_train / 255` (Normalization)**: This is one of the most important steps.
      * The original images are grayscale, with pixel values from 0 (black) to 255 (white).
      * By dividing by 255, you **scale** all pixel values to be between **0.0 and 1.0**.
      * Neural networks train much faster and more reliably on small floating-point numbers than on large integers.

-----

### c. Define the Network Architecture

```python
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128, activation = 'relu'),
    keras.layers.Dense(10, activation = 'softmax')
])
model.summary()
```

This is the "blueprint" for your model's brain.

  * **`model = keras.Sequential([...])`**: You're creating a `Sequential` model, which is the simplest kind: a stack of layers, one after the other.

  * **`keras.layers.Flatten(input_shape=(28,28))`**: This is the **Input Layer**.

      * Your images are 28x28 pixel grids.
      * A `Dense` (feedforward) layer only understands a flat, 1D list of numbers.
      * `Flatten` unrolls this 28x28 grid into a single, flat line of **784** pixels ($28 \times 28 = 784$).

  * **`keras.layers.Dense(128, activation = 'relu')`**: This is your **Hidden Layer**.

      * `Dense` means every neuron in this layer is connected to every one of the 784 inputs from the `Flatten` layer.
      * `128`: This is the number of neurons in the layer. It's a "hyperparameter" you can tune.
      * `activation = 'relu'`: The "Rectified Linear Unit" activation. It's a simple function that introduces non-linearity, allowing the model to learn complex patterns. It's like an "on/off" switch for the neuron.

  * **`keras.layers.Dense(10, activation = 'softmax')`**: This is the **Output Layer**.

      * `10`: It has 10 neurons, one for each possible class (the digits 0, 1, 2, ... 9).
      * `activation = 'softmax'`: This special function converts the raw scores from the 10 neurons into a **probability distribution** that sums to 1. For example, it will output something like `[0.01, 0.05, 0.0, 0.9, 0.02, ...]`, meaning it's 90% sure the image is a "3".

  * **`model.summary()`**: This prints the table showing your architecture. It confirms you have 101,770 "trainable parameters" (the weights and biases) that the model will adjust during training.

-----

### d. Train the Model

```python
model.compile(optimizer = 'sgd',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=10)
```

This is where the learning actually happens.

1.  **`model.compile(...)`**: This "configures" the model for training.
      * `optimizer = 'sgd'`: You're telling the model to use **Stochastic Gradient Descent**. This is the algorithm that updates the model's parameters to minimize the error.
      * `loss = 'sparse_categorical_crossentropy'`: You're choosing the **loss function**. This is how the model measures "how wrong" its prediction was. This specific one is the standard for multi-class classification (like yours).
      * `metrics = ['accuracy']`: You're telling the model to please report its **accuracy** (the percentage of images it gets right) as it trains.
2.  **`history = model.fit(...)`**: This command starts the training.
      * It "fits" the `x_train` (images) to the `y_train` (labels).
      * `epochs=10`: The model will pass through the *entire* 60,000-image training set 10 times.
      * `validation_data=(x_test,y_test)`: After each epoch, it will *evaluate* its performance on the 10,000-image test set. This is crucial—it shows you how well the model is **generalizing** to new data, not just "memorizing" the training data.
      * `history = ...`: You smartly save the output of the training process into a variable called `history`, which contains the loss and accuracy values from each epoch.

-----

### e. Evaluate the Network

```python
test_loss, test_acc = model.evaluate(x_test, y_test)
print('loss=%.3f' %test_loss)
print('Accuracy=%.3f' %test_acc)
...
predicted_value=model.predict(x_test)
print('predicted value: ', predicted_value[n])
```

This section checks the model's final performance.

1.  **`model.evaluate(x_test, y_test)`**: This runs a final, definitive evaluation on the test set.
      * The output `Accuracy=0.954` is your final "grade." It means your model, on data it has never seen before, can correctly identify the digit **95.4%** of the time. This is a great result\!
2.  **Making a Single Prediction:**
      * `n=random.randint(0,9999)`: You pick a random image index (e.g., `n` might be 785).
      * `plt.imshow(x_test[n])`: You display that random image.
      * `predicted_value=model.predict(x_test)`: This actually gets predictions for *all* 10,000 test images.
      * `print('predicted value: ', predicted_value[n])`: You then look up the prediction for your *specific* random image. The output array `[... 9.9510491e-01 ...]` is the `softmax` probability list. The 8th number (at index 7) is `0.995`, which means the model is **99.5% confident** that the image is a **7**. (And the image you plotted was, in fact, a 7\!)

-----

### f. Plot the Training Loss and Accuracy

```python
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
...
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
```

This is the final step, where you use the `history` object from `model.fit()` to visualize the training process.

  * **Accuracy Plot**: You plot the `accuracy` (on training data) and `val_accuracy` (on validation/test data). You can see both lines rise, which is good\! They are also very close, which means your model is **not overfitting** (it's not just "memorizing" the training set).
  * **Loss Plot**: You plot the `loss` (training error) and `val_loss` (validation/test error). You can see both lines go down, which is also excellent. This confirms the model became more accurate and less "wrong" with each epoch.

You have successfully implemented the entire machine learning workflow: data preparation, model building, model training, evaluation, and visualization.

Would you like to try modifying this model, for example by adding another hidden layer or using a different optimizer like 'adam' to see if you can improve that 95.4% accuracy?