# Deep Learning Lab 2: Building Your First Neural Network (MNIST Digit Classification)

## üìö What This Lab Is About
This lab teaches you how to build a simple neural network from scratch to recognize handwritten digits (0-9). It's like teaching a computer to read numbers the way humans do! This is often called the "Hello World" of deep learning because it's simple enough to understand but complex enough to be interesting.

---

## üéØ The Big Picture

**The Goal:** Train a computer to look at an image of a handwritten digit and correctly identify which number (0-9) it represents.

**The Dataset:** MNIST - A collection of 70,000 images of handwritten digits
- 60,000 for training (teaching the model)
- 10,000 for testing (seeing how well it learned)

---

## üìñ Step-by-Step Explanation

### Step 1: Importing Necessary Packages

```python
import tensorflow as tf
from tensorflow import keras 
import matplotlib.pyplot as plt
import random
```

**What's happening here?**
- **tensorflow**: The main deep learning framework we're using
- **keras**: The user-friendly interface to build our neural network
- **matplotlib.pyplot**: A library to create charts and display images
- **random**: To pick random samples from our data

**Real-world analogy:** Think of this like gathering all your tools before starting a craft project - you need your scissors, glue, paper, etc.

---

### Step 2: Loading the MNIST Dataset

```python
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

**What's happening here?**
1. **mnist**: Getting access to the MNIST dataset that comes pre-packaged with Keras
2. **x_train**: 60,000 training images (the pictures of digits)
3. **y_train**: 60,000 training labels (the actual numbers: 0, 1, 2, ... 9)
4. **x_test**: 10,000 testing images
5. **y_test**: 10,000 testing labels

**What do the images look like?**
- Each image is 28 pixels wide √ó 28 pixels tall
- Each pixel has a value from 0 (black) to 255 (white)
- Gray values in between represent different shades

**Real-world analogy:** Imagine a teacher preparing flashcards:
- **Training set**: Flashcards used to teach students
- **Testing set**: Different flashcards used to quiz students and see if they actually learned

---

### Step 3: Normalizing/Scaling the Data

```python
x_train = x_train / 255
x_test = x_test / 255
```

**What's happening here?**
We're converting pixel values from the range [0, 255] to [0, 1] by dividing by 255.

**Why do we do this?**
- Neural networks learn better when input values are small
- It's like converting temperatures from Fahrenheit to a 0-1 scale - easier to work with
- This process is called "normalization" or "scaling"

**Example:**
- Original pixel value: 200 (bright)
- After normalization: 200/255 = 0.784
- Original pixel value: 50 (dark)
- After normalization: 50/255 = 0.196

---

### Step 4: Defining the Network Architecture

```python
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28,28)),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10, activation='softmax')
])
```

**What's happening here?**
We're building a neural network with three layers. Let's break down each one:

#### Layer 1: Flatten (Input Layer)
```python
keras.layers.Flatten(input_shape=(28,28))
```
- **Purpose**: Convert the 2D image (28√ó28 pixels) into a 1D array (784 numbers in a row)
- **Input**: 28√ó28 = 784 pixels
- **Output**: 784 neurons (one for each pixel)
- **Analogy**: Imagine unrolling a rolled-up poster to make it flat

#### Layer 2: Dense/Hidden Layer
```python
keras.layers.Dense(128, activation='relu')
```
- **Dense**: Every neuron is connected to every neuron in the previous layer
- **128**: This layer has 128 neurons
- **activation='relu'**: ReLU (Rectified Linear Unit) - more on this below
- **Purpose**: Learn patterns and features from the pixel data

**What is ReLU?**
- ReLU is a simple function: If input is negative, output 0; if positive, keep it as is
- Formula: f(x) = max(0, x)
- Example: -5 ‚Üí 0, 3 ‚Üí 3, 10 ‚Üí 10
- **Why use it?** It helps the network learn complex patterns and makes training faster

#### Layer 3: Output Layer
```python
keras.layers.Dense(10, activation='softmax')
```
- **10 neurons**: One for each digit (0-9)
- **activation='softmax'**: Converts outputs to probabilities

**What is Softmax?**
- Takes a list of numbers and converts them to probabilities that sum to 1
- Example output: [0.05, 0.02, 0.15, 0.60, 0.03, 0.10, 0.01, 0.02, 0.01, 0.01]
  - Index 3 has 0.60 (60%) ‚Üí The network is 60% confident it's a "3"
  - Index 2 has 0.15 (15%) ‚Üí 15% confident it's a "2"
  - And so on...

**Model Summary:**
```
Total parameters: 101,770
- Flatten layer: 0 parameters (just reshapes data)
- Hidden layer: 100,480 parameters (784 √ó 128 + 128 bias terms)
- Output layer: 1,290 parameters (128 √ó 10 + 10 bias terms)
```

**Real-world analogy:** 
Think of the neural network like a factory assembly line:
1. **Flatten**: Raw materials enter the factory
2. **Hidden Layer**: Workers process and extract features
3. **Output Layer**: Final inspection - which product category does this belong to?

---

### Step 5: Training the Model Using SGD

```python
model.compile(optimizer='sgd',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(x_train, y_train, 
                   validation_data=(x_test, y_test), 
                   epochs=10)
```

**What's happening here?**

#### compile() - Setting Up the Learning Process
Think of this as configuring the rules before playing a game.

**optimizer='sgd'** (Stochastic Gradient Descent)
- SGD is the learning algorithm - it's how the network improves
- **How it works:** 
  1. Make a prediction
  2. Calculate how wrong it was (error)
  3. Adjust the weights (parameters) to reduce the error
  4. Repeat millions of times
- **Analogy**: Like practicing free throws in basketball - each miss teaches you how to adjust your aim

**loss='sparse_categorical_crossentropy'**
- This measures how wrong the predictions are
- "Cross-entropy" is a fancy way of measuring the difference between:
  - What the model predicted
  - What the actual answer was
- Lower loss = better performance
- **Analogy**: Like a grade on a test - you want it to be as high (or in this case, as low) as possible

**metrics=['accuracy']**
- Track what percentage of predictions are correct
- Easy to understand: 95% accuracy = 95 out of 100 predictions were right

#### fit() - Actually Training the Model
This is where the learning happens!

**x_train, y_train**: The training data (images and their labels)

**validation_data=(x_test, y_test)**: 
- Check performance on unseen data after each epoch
- Helps detect if the model is "memorizing" instead of "learning"

**epochs=10**: 
- An epoch is one complete pass through all training data
- 10 epochs = the model sees all 60,000 images 10 times
- Each time it sees them, it learns a bit more

**Training Progress (what the output means):**
```
Epoch 1/10: loss: 0.6385 - accuracy: 0.8437 - val_loss: 0.3495 - val_accuracy: 0.9069
```
- **Epoch 1**: First time through the data
- **loss: 0.6385**: Training error (we want this to go down)
- **accuracy: 0.8437**: 84.37% accuracy on training data
- **val_loss: 0.3495**: Error on test data
- **val_accuracy: 0.9069**: 90.69% accuracy on test data

**What we observe:**
- Loss decreases from 0.64 ‚Üí 0.16 (getting better!)
- Accuracy increases from 84% ‚Üí 95% (learning!)
- Takes about 9 seconds per epoch

---

### Step 6: Evaluating the Network

```python
test_loss, test_acc = model.evaluate(x_test, y_test)
print('loss=%.3f' % test_loss)
print('Accuracy=%.3f' % test_acc)
```

**What's happening here?**
- **evaluate()**: Tests the model on data it has never seen before
- **test_loss**: How wrong the predictions are (0.161)
- **test_acc**: What percentage are correct (95.24%)

**Results:**
```
loss=0.161
Accuracy=0.952
```

**What does this mean?**
- The model correctly identifies 95.2% of handwritten digits
- That's 9,524 correct out of 10,000 test images
- Pretty impressive for such a simple network!

**Real-world analogy:** 
This is like giving a final exam to a student. The training was like studying, and now we're seeing how well they actually learned the material (not just memorized it).

---

### Step 7: Making Predictions on Individual Images

```python
n = random.randint(0, 9999)
plt.imshow(x_test[n])
plt.show()

predicted_value = model.predict(x_test)
print('predicted value: ', predicted_value[n])
```

**What's happening here?**
1. Pick a random test image (between 0 and 9,999)
2. Display the image so we can see it
3. Get the model's prediction for all test images
4. Show the prediction for our random image

**Understanding the Output:**
```
predicted value: [3.7e-04, 5.8e-04, 2.9e-02, 7.4e-03, 6.9e-03, 
                  7.5e-01, 6.1e-04, 1.4e-02, 1.8e-02, 1.7e-01]
```

**Breaking this down:**
- This is an array of 10 probabilities (one for each digit 0-9)
- Index 0: 0.037% chance it's a "0"
- Index 1: 0.058% chance it's a "1"
- Index 2: 2.9% chance it's a "2"
- Index 5: **75%** chance it's a "5" ‚Üê Highest! This is the prediction
- Index 9: 17% chance it's a "9" (second most likely)

**Conclusion:** The model is 75% confident this is the digit "5"

---

### Step 8: Plotting Training History

```python
# Plotting accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plotting loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()
```

**What's happening here?**
Creating visualizations to see how the model improved during training.

**Accuracy Plot:**
- X-axis: Epochs (1-10)
- Y-axis: Accuracy (0-100%)
- Blue line: Training accuracy (keeps going up!)
- Orange line: Validation accuracy (also going up!)

**What to look for:**
- Both lines should go up (good!)
- Lines close together = model generalizes well
- If training accuracy is way higher than validation = overfitting (memorizing instead of learning)

**Loss Plot:**
- X-axis: Epochs (1-10)
- Y-axis: Loss (error)
- Both lines should go down (getting better!)

---

## üîç Key Concepts Explained

### What is a Neural Network?
A neural network is inspired by how the human brain works:
- **Neurons**: Simple processing units that take inputs, process them, and produce outputs
- **Layers**: Groups of neurons that work together
- **Connections**: How information flows from one layer to the next
- **Weights**: Numbers that determine how strong each connection is
- **Learning**: Adjusting these weights to make better predictions

### Forward Pass (Making a Prediction)
1. Input image enters (784 pixels)
2. First layer processes it (128 neurons think about the patterns)
3. Output layer makes a decision (which digit is it?)
4. We get 10 probabilities (one for each digit)

### Backward Pass (Learning)
1. Compare prediction to actual answer
2. Calculate error
3. Go backward through the network
4. Adjust weights to reduce error
5. Repeat for next image

This is called "backpropagation" - it's how neural networks learn!

---

## üéì Important Terminology

| Term | Simple Explanation |
|------|-------------------|
| **Epoch** | One complete pass through all training data |
| **Batch** | A subset of data processed together |
| **Loss** | How wrong the predictions are (lower is better) |
| **Accuracy** | Percentage of correct predictions (higher is better) |
| **Training** | Learning from labeled examples |
| **Validation** | Testing on unseen data to check if learning is real |
| **Overfitting** | Memorizing training data instead of learning patterns |
| **Underfitting** | Not learning enough from the data |
| **Weights** | Numbers the network adjusts during learning |
| **Bias** | Additional adjustable parameters in each neuron |

---

## üí° Why This Matters

This simple digit recognition system demonstrates the foundation for:

1. **Handwriting Recognition**: Digital note-taking apps, postal mail sorting
2. **Check Processing**: Banks automatically reading handwritten amounts
3. **Form Processing**: Automatically digitizing paper forms
4. **Educational Apps**: Apps that grade handwritten math homework

The same principles scale up to:
- **Face Recognition**: Unlocking your phone with your face
- **Medical Diagnosis**: Detecting diseases from X-rays
- **Self-Driving Cars**: Recognizing traffic signs and pedestrians
- **Language Translation**: Google Translate

---

## üöÄ Key Takeaways

1. **Data Preparation is Crucial**: We normalized our data (divided by 255) to help the network learn better

2. **Architecture Matters**: 
   - Flatten layer: Reshapes input
   - Hidden layer: Learns features
   - Output layer: Makes predictions

3. **Training Takes Time**: 10 epochs through 60,000 images teaches the network patterns

4. **Validation is Important**: Testing on unseen data tells us if the model really learned or just memorized

5. **95% Accuracy is Great**: For such a simple network, being right 95% of the time is impressive!

6. **SGD is Powerful**: This simple optimization algorithm can teach computers to recognize patterns

---

## üîß What Could We Improve?

1. **More Hidden Layers**: Adding depth could improve accuracy
2. **More Neurons**: Wider layers might capture more patterns
3. **Different Optimizer**: Adam or RMSprop might train faster than SGD
4. **Data Augmentation**: Rotating/shifting images to create more training data
5. **Regularization**: Techniques like Dropout to prevent overfitting
6. **Convolutional Layers**: Better for image data (we'll see this in the next lab!)

---

## üéØ Summary

You've just built your first neural network! Here's what we did:

1. ‚úÖ Loaded 70,000 images of handwritten digits
2. ‚úÖ Normalized the pixel values for better training
3. ‚úÖ Built a simple 3-layer neural network
4. ‚úÖ Trained it using SGD optimization
5. ‚úÖ Achieved 95% accuracy on unseen test data
6. ‚úÖ Made predictions on individual images
7. ‚úÖ Visualized the training progress

**Congratulations!** You now understand the fundamental building blocks of deep learning. Every complex AI system builds on these same principles - they just add more layers, more sophisticated architectures, and more training data.

The journey from here involves learning about:
- Convolutional Neural Networks (CNNs) for images
- Recurrent Neural Networks (RNNs) for sequences
- Transformers for language
- And many more exciting architectures!

Keep learning and experimenting! üöÄ

