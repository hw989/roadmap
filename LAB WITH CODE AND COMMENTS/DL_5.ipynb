{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "# - numpy/pandas: array and table utilities\n",
    "# - tensorflow/keras: tokenization, sequences, one-hot labels, and model layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e596b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small sample corpus (text) and split it into tokens (words)\n",
    "# You can replace this with your own sentences\n",
    "# dl_data will be a list of tokens (strings)\n",
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised. \n",
    "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
    "\"\"\"\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae494900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 75\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('neural', 4), ('and', 5), ('as', 6), ('of', 7), ('machine', 8), ('supervised', 9), ('have', 10)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus and prepare mappings between words and ids\n",
    "# Tokenizer builds a vocabulary and assigns an id to each unique token\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "# Add a PAD token (id=0) to pad context windows to a fixed length\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "\n",
    "# Convert each token to its id (each 'doc' here is a single token from dl_data)\n",
    "wids = [[word2id[w] for w in text_to_word_sequence(doc)] for doc in dl_data]\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d236309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (context, target) training pairs for CBOW\n",
    "# For each center word, take 'window_size' words on the left and right as context\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2  # total number of context words\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            # collect neighbors around the center index (skip the center itself)\n",
    "            context_words = [words[i] for i in range(index - window_size, index + window_size + 1)\n",
    "                             if i != index and 0 <= i < sentence_length]\n",
    "            label_word = [word]  # the center word is the target\n",
    "            # pad context to fixed length and one-hot encode the target\n",
    "            x = pad_sequences([context_words], maxlen=context_length)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "\n",
    "# Preview: print a few generated (context, target) pairs\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "    # Optionally skip samples that include PAD (id 0)\n",
    "    if 0 not in x[0]:  # Example filter (remove PAD if desired)\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argmax(y[0])])\n",
    "        i += 1\n",
    "    if i == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d4937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,575</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │           \u001b[38;5;34m7,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lambda_8 (\u001b[38;5;33mLambda\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m)                  │           \u001b[38;5;34m7,575\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,075</span> (58.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,075\u001b[0m (58.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,075</span> (58.89 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,075\u001b[0m (58.89 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a simple CBOW model:\n",
    "# - Embedding: look up dense vectors for each context word id\n",
    "# - Lambda(mean): average the context embeddings to get one vector\n",
    "# - Dense + softmax: predict the target word distribution over the vocabulary\n",
    "import tensorflow as tf\n",
    "\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: tf.reduce_mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "# Train with categorical crossentropy and RMSProp optimizer\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# Build and show the model summary\n",
    "cbow.build(input_shape=(None, window_size * 2))\n",
    "cbow.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dac9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 430.84808\n",
      "\n",
      "Epoch: 2 \tLoss: 430.3429\n",
      "\n",
      "Epoch: 3 \tLoss: 428.4553\n",
      "\n",
      "Epoch: 4 \tLoss: 426.678\n",
      "\n",
      "Epoch: 5 \tLoss: 425.2011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the CBOW model with a simple manual loop\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0.0\n",
    "    i = 0\n",
    "    # Stream (context, target) pairs from the generator\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        # One mini-update per pair\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        # Optional progress print for very large corpora\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf5b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.053811</td>\n",
       "      <td>-0.021965</td>\n",
       "      <td>-0.031946</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>-0.011971</td>\n",
       "      <td>0.017166</td>\n",
       "      <td>-0.033597</td>\n",
       "      <td>0.037855</td>\n",
       "      <td>-0.028029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>0.059729</td>\n",
       "      <td>-0.015220</td>\n",
       "      <td>0.031792</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.030792</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.011492</td>\n",
       "      <td>0.003695</td>\n",
       "      <td>-0.038793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>0.012211</td>\n",
       "      <td>0.022778</td>\n",
       "      <td>-0.026541</td>\n",
       "      <td>0.045527</td>\n",
       "      <td>0.051432</td>\n",
       "      <td>0.037090</td>\n",
       "      <td>-0.026177</td>\n",
       "      <td>-0.049971</td>\n",
       "      <td>0.034592</td>\n",
       "      <td>-0.008757</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005389</td>\n",
       "      <td>0.012320</td>\n",
       "      <td>-0.004715</td>\n",
       "      <td>-0.009161</td>\n",
       "      <td>-0.020322</td>\n",
       "      <td>0.061598</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>0.008160</td>\n",
       "      <td>-0.036231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural</th>\n",
       "      <td>0.007575</td>\n",
       "      <td>-0.005314</td>\n",
       "      <td>0.006753</td>\n",
       "      <td>0.020443</td>\n",
       "      <td>-0.004717</td>\n",
       "      <td>-0.004553</td>\n",
       "      <td>0.046599</td>\n",
       "      <td>-0.002272</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>-0.004326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012735</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>-0.033299</td>\n",
       "      <td>0.045449</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.009265</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.041288</td>\n",
       "      <td>-0.025355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.013518</td>\n",
       "      <td>-0.024830</td>\n",
       "      <td>-0.029084</td>\n",
       "      <td>-0.006710</td>\n",
       "      <td>-0.045961</td>\n",
       "      <td>0.029382</td>\n",
       "      <td>-0.013573</td>\n",
       "      <td>0.010873</td>\n",
       "      <td>-0.038445</td>\n",
       "      <td>-0.035782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027289</td>\n",
       "      <td>-0.013834</td>\n",
       "      <td>-0.040606</td>\n",
       "      <td>-0.040892</td>\n",
       "      <td>-0.010782</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>-0.002082</td>\n",
       "      <td>0.031735</td>\n",
       "      <td>0.020708</td>\n",
       "      <td>0.030787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>-0.049720</td>\n",
       "      <td>0.025355</td>\n",
       "      <td>0.039353</td>\n",
       "      <td>0.017494</td>\n",
       "      <td>-0.044821</td>\n",
       "      <td>-0.004506</td>\n",
       "      <td>0.032198</td>\n",
       "      <td>-0.041671</td>\n",
       "      <td>0.014883</td>\n",
       "      <td>0.009515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031630</td>\n",
       "      <td>0.021036</td>\n",
       "      <td>0.012921</td>\n",
       "      <td>-0.013501</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>-0.003683</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>-0.006832</td>\n",
       "      <td>0.021592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "deep     -0.053811 -0.021965 -0.031946  0.025899  0.025870 -0.011971   \n",
       "networks  0.012211  0.022778 -0.026541  0.045527  0.051432  0.037090   \n",
       "neural    0.007575 -0.005314  0.006753  0.020443 -0.004717 -0.004553   \n",
       "and       0.013518 -0.024830 -0.029084 -0.006710 -0.045961  0.029382   \n",
       "as       -0.049720  0.025355  0.039353  0.017494 -0.044821 -0.004506   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "deep      0.017166 -0.033597  0.037855 -0.028029  ...  0.028481  0.059729   \n",
       "networks -0.026177 -0.049971  0.034592 -0.008757  ... -0.005389  0.012320   \n",
       "neural    0.046599 -0.002272  0.002667 -0.004326  ... -0.012735  0.042071   \n",
       "and      -0.013573  0.010873 -0.038445 -0.035782  ... -0.027289 -0.013834   \n",
       "as        0.032198 -0.041671  0.014883  0.009515  ... -0.031630  0.021036   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "deep     -0.015220  0.031792  0.011434  0.030792  0.000047  0.011492   \n",
       "networks -0.004715 -0.009161 -0.020322  0.061598  0.064920  0.016656   \n",
       "neural   -0.033299  0.045449 -0.000771  0.009265 -0.014977  0.001748   \n",
       "and      -0.040606 -0.040892 -0.010782  0.004321 -0.002082  0.031735   \n",
       "as        0.012921 -0.013501  0.013961  0.013314 -0.003683 -0.030884   \n",
       "\n",
       "                98        99  \n",
       "deep      0.003695 -0.038793  \n",
       "networks  0.008160 -0.036231  \n",
       "neural    0.041288 -0.025355  \n",
       "and       0.020708  0.030787  \n",
       "as       -0.006832  0.021592  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the learned word embeddings from the model\n",
    "weights = cbow.get_weights()[0]\n",
    "\n",
    "# Remove the PAD row (index 0) so it aligns with words starting from id=1\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "# Show a small table of embeddings: rows=words, columns=embedding dimensions\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cff7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Empty cell) – use this space for extra experiments or notes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
