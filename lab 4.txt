Use Autoencoder to implement anomaly detection. Build the model by using:
a. Import required libraries
b. Upload / access the dataset
c. Encoder converts it into latent representation
d. Decoder networks convert it back to the original input
e. Compile the models with Optimizer, Loss, and Evaluation Metrics

//// code explaination
Here is a step-by-step explanation of your code.

The high-level goal of this script is to build a special type of neural network called an **Autoencoder** to detect anomalies in ECG (electrocardiogram) data.

The core idea is:
1.  **Train** the model *only* on **normal** ECG signals.
2.  The model learns to be very good at "reconstructing" (compressing and then decompressing) these normal signals.
3.  When the model is shown a **new** signal, it will try to reconstruct it.
    * If the new signal is **normal**, the reconstruction will be accurate, and the **error** (the difference between the original and the reconstruction) will be **low**.
    * If the new signal is an **anomaly**, the model will struggle to reconstruct it (since it never learned how), and the **error** will be **high**.
4.  By setting a "threshold" for this error, you can classify signals as normal or anomalous.

---

### 1. Data Preparation and Splitting

* **Libraries and Dataset:** You import all necessary libraries (`pandas`, `tensorflow`, `sklearn`, etc.) and load the ECG dataset from a URL.
* **Features and Target:**
    * `features = data.drop(140, axis=1)`: You select all columns (0-139) as the input features. Each row is a single ECG signal with 140 time-step measurements.
    * `target = data[140]`: You select the last column (140) as the label. In this dataset, **1.0 means "Normal"** and **0.0 means "Anomaly"**.
* **Train/Test Split:**
    * `x_train, x_test, y_train, y_test = train_test_split(...)`: You split the data into training and testing sets. `stratify=target` is important: it ensures that the training and test sets have the same percentage of normal (1.0) and anomalous (0.0) samples as the original dataset.

---

### 2. The "Normal-Only" Training Strategy

This is the most critical part of your preprocessing for anomaly detection:

* `train_index = y_train[y_train == 1].index`: You look at your training labels (`y_train`) and get the index numbers for *only* the samples that are labeled `1.0` (Normal).
* `train_data = x_train.loc[train_index]`: You use these indices to create a new training dataset `train_data` that contains **only the normal ECG signals**. The anomalous (0.0) samples are deliberately excluded from training.

### 3. Data Scaling

* `min_max_scaler = MinMaxScaler(feature_range=(0, 1))`: You create a scaler that will squish all feature values to be between 0 and 1.
* `x_train_scaled = min_max_scaler.fit_transform(train_data.copy())`: You **fit** the scaler on the `train_data` (the normal-only data). This learns the minimum and maximum values for scaling. Then, it transforms the data.
* `x_test_scaled = min_max_scaler.transform(x_test.copy())`: You use the **same** scaler (already fitted on the normal data) to transform the `x_test` data. This is crucial to prevent data leakage and ensure the test data is scaled in the exact same way as the training data.

---

### 4. Building the Autoencoder Model

You define a custom `AutoEncoder` class. An autoencoder has two parts:

* **a. Encoder (Your Point c):** This part compresses the input data into a smaller "latent representation."
    * Your encoder (`self.encoder`) is a `Sequential` model that takes the 140 features and compresses them down through layers: `Dense(64)` -> `Dense(32)` -> `Dense(16)` -> `Dense(8)`.
    * The final `Dense(8)` layer is the **bottleneck**. It forces the network to learn a compressed representation (8 numbers) of the original 140-number signal.

* **b. Decoder (Your Point d):** This part tries to reconstruct the *original* input from the compressed (latent) representation.
    * Your decoder (`self.decoder`) is a mirror image of the encoder. It takes the 8-number representation and expands it back up: `Dense(16)` -> `Dense(32)` -> `Dense(64)` -> `Dense(140)`.
    * The final `Dense(140, activation='sigmoid')` layer outputs 140 numbers. The `sigmoid` activation is used because you scaled your input data between 0 and 1.

* **`call(self, inputs)`:** This method defines what happens when you run the model. It simply runs the `encoder` on the `inputs` and then runs the `decoder` on the `encoded` output.

---

### 5. Compiling and Training (Your Point e)

* **Model Configuration:**
    * `model = AutoEncoder(output_units=x_train_scaled.shape[1])`: You create an instance of your model, telling it the output dimension must be 140.
    * `model.compile(loss='msle', optimizer='adam', metrics=['mse'])`:
        * **`optimizer='adam'`**: An efficient algorithm for updating the model's weights.
        * **`loss='msle'` (Mean Squared LogarithmicError)**: This is the function that measures how "wrong" the model's reconstruction is. It's a good choice for data that isn't perfectly normally distributed.
        * **`metrics=['mse']`**: You also ask it to track Mean Squared Error, just for extra information.

* **Model Training:**
    * `history = model.fit(...)`: You start the training.
    * `x_train_scaled, x_train_scaled`: This is the key. You provide the **same data** as both the input (`x_train_scaled`) and the target (`x_train_scaled`). You are teaching the model: "When you see this input, you must learn to output the exact same thing."
    * `validation_data=(x_test_scaled, x_test_scaled)`: After each epoch, the model will test its reconstruction ability on the `x_test_scaled` data (which contains *both* normal and anomalous samples) and report the `val_loss`.

* **Plotting:** The `plt.plot` code visualizes the `loss` (on training data) and `val_loss` (on test data) over the 20 epochs. This helps you see if the model is learning correctly.

---

### 6. Finding the Anomaly Threshold

Now that the model is trained (only on normal data), you need to define what "high error" means.

* **`find_threshold` function:**
    1.  `reconstructions = model.predict(x_train_scaled)`: Get the model's reconstructions for all the *normal training data*.
    2.  `reconstruction_errors = tf.keras.losses.msle(...)`: Calculate the reconstruction error (`msle`) for *each* normal sample.
    3.  `threshold = np.mean(...) + np.std(...)`: You define the threshold statistically. You calculate the **average error** for normal data and **add one standard deviation**. The logic is: "Most normal samples should have an error below this value. Anything significantly higher is suspicious."

---

### 7. Making Predictions and Evaluating

* **`get_predictions` function:**
    1.  `predictions = model.predict(x_test_scaled)`: Get the model's reconstructions for the *test data*.
    2.  `errors = tf.keras.losses.msle(...)`: Calculate the reconstruction error for *each* test sample.
    3.  `anomaly_mask = pd.Series(errors) > threshold`: This is the detection step. It checks if each sample's error is **greater than the threshold**. This results in a list of `True` (it's an anomaly) or `False` (it's normal).
    4.  `preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)`: This converts the `True`/`False` mask into the 0.0/1.0 labels used by the dataset.
        * If `True` (anomaly), map to `0.0`.
        * If `False` (normal), map to `1.0`.

* **Final Accuracy:**
    * `predictions = get_predictions(...)`: You run the prediction function.
    * `accuracy_score(predictions, y_test)`: You compare your model's predictions (the 0s and 1s) to the *actual* labels (`y_test`) from the test set.

The final output of **0.956** means your autoencoder model successfully identified normal and anomalous ECG signals with **95.6% accuracy** on the unseen test data.